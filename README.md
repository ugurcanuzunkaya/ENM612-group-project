# Max-Min Separability Projesi

Bu proje, Adil Hoca'nÄ±n makalesinde Ã¶nerilen **Max-Min Separability** algoritmasÄ±nÄ±n Python ile implementasyonunu iÃ§erir. Proje, **Test Driven Development (TDD)** prensiplerine sadÄ±k kalÄ±narak geliÅŸtirilmiÅŸ ve optimizasyon sÃ¼reÃ§leri iÃ§in **Gurobi** Ã§Ã¶zÃ¼cÃ¼sÃ¼ kullanÄ±lmÄ±ÅŸtÄ±r.

## ğŸ“‹ Ä°Ã§indekiler
- [Proje HakkÄ±nda](#proje-hakkÄ±nda)
- [Kurulum](#kurulum)
- [KullanÄ±m](#kullanÄ±m)
- [Proje YapÄ±sÄ±](#proje-yapÄ±sÄ±)
- [Algoritma DetaylarÄ±](#algoritma-detaylarÄ±)
- [SonuÃ§larÄ±n Analizi](#sonuÃ§larÄ±n-analizi)
- [Gelecek Ã‡alÄ±ÅŸmalar](#gelecek-Ã§alÄ±ÅŸmalar)

## ğŸš€ Proje HakkÄ±nda
Bu Ã§alÄ±ÅŸma, lineer olmayan veri setlerini (Ã¶rneÄŸin `make_moons`) ayÄ±rmak iÃ§in parÃ§alÄ± lineer (piecewise linear) hiperdÃ¼zlemler kullanan bir sÄ±nÄ±flandÄ±rma yÃ¶ntemidir. YÃ¶ntem, klasik SVM veya Lojistik Regresyon'dan farklÄ± olarak, her sÄ±nÄ±f iÃ§in birden fazla hiperdÃ¼zlem grubu (polyhedral sets) tanÄ±mlar ve **Max-Min** mantÄ±ÄŸÄ±yla en iyi ayrÄ±mÄ± yapmaya Ã§alÄ±ÅŸÄ±r.

TÃ¼revsiz optimizasyon (Derivative-Free Optimization) yÃ¶ntemlerinden biri olan **Discrete Gradient Method (DGM)** kullanÄ±lmÄ±ÅŸtÄ±r. Ä°niÅŸ yÃ¶nÃ¼nÃ¼ bulmak iÃ§in ise Gurobi ile bir Kuadratik Programlama (QP) alt problemi Ã§Ã¶zÃ¼lmektedir.

## ğŸ›  Kurulum

Bu projeyi Ã§alÄ±ÅŸtÄ±rmak iÃ§in sisteminizde Python ve Gurobi lisansÄ±nÄ±n yÃ¼klÃ¼ olmasÄ± gerekir. Proje baÄŸÄ±mlÄ±lÄ±klarÄ± `uv` paket yÃ¶neticisi ile yÃ¶netilmektedir.

### AdÄ±m 1: Projeyi KlonlayÄ±n
```bash
git clone <repo-url>
cd ENM612-group-project
```

### AdÄ±m 2: BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kleyin
EÄŸer `uv` yÃ¼klÃ¼ deÄŸilse, Ã¶nce onu yÃ¼kleyin veya standart `pip` kullanÄ±n.
```bash
# uv ile kurulum (Ã–nerilen)
uv sync

# Veya pip ile
pip install numpy matplotlib gurobipy scikit-learn
```

**Ã–nemli Not:** Gurobi lisansÄ±nÄ±zÄ±n versiyonu ile `gurobipy` kÃ¼tÃ¼phanesinin versiyonunun uyumlu olduÄŸundan emin olun. (Bu projede 12.0.3 versiyonu kullanÄ±lmÄ±ÅŸtÄ±r).

## ğŸ’» KullanÄ±m

### Testleri Ã‡alÄ±ÅŸtÄ±rma (TDD)
Kodun doÄŸruluÄŸunu teyit etmek iÃ§in birim testleri Ã§alÄ±ÅŸtÄ±rabilirsiniz:
```bash
uv run pytest tests/test_max_min.py
```
Bu testler; hiperparametrelerin doÄŸruluÄŸunu, kayÄ±p fonksiyonunun negatif olmamasÄ±nÄ± ve gradyan boyutlarÄ±nÄ± kontrol eder.

### Modeli EÄŸitme ve GÃ¶rselleÅŸtirme
Modeli `make_moons` veri seti Ã¼zerinde eÄŸitmek ve karar sÄ±nÄ±rlarÄ±nÄ± Ã§izdirmek iÃ§in:
```bash
uv run main.py
```
Bu komut, eÄŸitimi baÅŸlatacak ve sonuÃ§ta `decision_boundary.png` adÄ±nda bir gÃ¶rsel oluÅŸturacaktÄ±r.

## ğŸ“‚ Proje YapÄ±sÄ±

```
.
â”œâ”€â”€ src/
â”‚   â””â”€â”€ max_min.py       # AlgoritmanÄ±n ana sÄ±nÄ±fÄ± (MaxMinSeparability)
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_max_min.py  # Birim testler
â”œâ”€â”€ main.py              # Ã‡alÄ±ÅŸtÄ±rma ve gÃ¶rselleÅŸtirme betiÄŸi
â”œâ”€â”€ pyproject.toml       # BaÄŸÄ±mlÄ±lÄ±k dosyasÄ±
â””â”€â”€ README.md            # DokÃ¼mantasyon
```

## ğŸ§  Algoritma DetaylarÄ±

Kodun temel bileÅŸenleri ÅŸunlardÄ±r:

1.  **Objective Function (AmaÃ§ Fonksiyonu):** Makaledeki Denklem 31 ve 32'nin vektÃ¶rize edilmiÅŸ halidir. Hata (Loss) deÄŸeri hesaplanÄ±rken, doÄŸru sÄ±nÄ±flandÄ±rÄ±lmÄ±ÅŸ ve "gÃ¼venli" bÃ¶lgedeki noktalar iÃ§in hata 0 kabul edilir (Hinge Loss benzeri yapÄ±).
2.  **Discrete Gradient (AyrÄ±k Gradyan):** Fonksiyonun tÃ¼revi alÄ±namadÄ±ÄŸÄ± iÃ§in (non-smooth), rastgele yÃ¶nlerdeki deÄŸiÅŸimlere bakÄ±larak gradyan tahmin edilir (TanÄ±m 2).
3.  **Direction Finding (YÃ¶n Bulma):** Elde edilen gradyan demetinin (bundle) konveks zarfÄ±nda orijine en yakÄ±n nokta bulunur. Bu nokta, en dik iniÅŸ yÃ¶nÃ¼nÃ¼n tersidir. Bu iÅŸlem Gurobi ile Ã§Ã¶zÃ¼lÃ¼r.

## ğŸ“Š SonuÃ§larÄ±n Analizi

`main.py` Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda elde edilen `decision_boundary.png` gÃ¶rseli ÅŸunlarÄ± gÃ¶sterir:
- **Mavi Noktalar:** A SÄ±nÄ±fÄ± (Min Region)
- **KÄ±rmÄ±zÄ± Noktalar:** B SÄ±nÄ±fÄ± (Max Region)
- **Kontur AlanlarÄ±:** Modelin karar sÄ±nÄ±rlarÄ±.

Model, `make_moons` gibi lineer ayrÄ±lamayan bir veri setini, birden fazla doÄŸru parÃ§asÄ± kullanarak baÅŸarÄ±yla ayÄ±rmaktadÄ±r. BaÅŸlangÄ±Ã§ta yÃ¼ksek olan hata deÄŸeri (Loss), iterasyonlar ilerledikÃ§e azalmakta ve 0'a yaklaÅŸmaktadÄ±r. Bu, algoritmanÄ±n yakÄ±nsadÄ±ÄŸÄ±nÄ± gÃ¶sterir.

## ğŸ”® Gelecek Ã‡alÄ±ÅŸmalar (Future Updates)

Bu proje ÅŸu an temel bir implementasyondur. Ä°leride yapÄ±labilecek geliÅŸtirmeler:

1.  **Hiperparametre Optimizasyonu:** `n_groups` ve `n_hyperplanes_per_group` parametrelerinin otomatik seÃ§imi iÃ§in Cross-Validation eklenebilir.
2.  **Daha HÄ±zlÄ± Ã‡Ã¶zÃ¼cÃ¼ler:** Gurobi yerine aÃ§Ä±k kaynaklÄ± Ã§Ã¶zÃ¼cÃ¼ler (Ã¶rneÄŸin OSQP veya SciPy) entegre edilerek lisans baÄŸÄ±mlÄ±lÄ±ÄŸÄ± azaltÄ±labilir.
3.  **BÃ¼yÃ¼k Veri DesteÄŸi:** Kod ÅŸu an tÃ¼m veriyi bellekte tutmaktadÄ±r. BÃ¼yÃ¼k veri setleri iÃ§in "Mini-batch" yaklaÅŸÄ±mÄ± eklenebilir.
4.  **Ã‡oklu SÄ±nÄ±f DesteÄŸi:** Åu an sadece ikili sÄ±nÄ±flandÄ±rma (Binary Classification) yapÄ±lmaktadÄ±r. One-vs-All yÃ¶ntemiyle Ã§oklu sÄ±nÄ±f desteÄŸi getirilebilir.

---
*Bu proje ENM612 dersi kapsamÄ±nda hazÄ±rlanmÄ±ÅŸtÄ±r.*
